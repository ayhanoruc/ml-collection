{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import train_data, test_data\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['good', 'bad', 'happy', 'sad', 'not good', 'not bad', 'not happy', 'not sad', 'very good', 'very bad', 'very happy', 'very sad', 'i am happy', 'this is good', 'i am bad', 'this is bad', 'i am sad', 'this is sad', 'i am not happy', 'this is not good', 'i am not bad', 'this is not sad', 'i am very happy', 'this is very good', 'i am very bad', 'this is very sad', 'this is very happy', 'i am good not bad', 'this is good not bad', 'i am bad not good', 'i am good and happy', 'this is not good and not happy', 'i am not at all good', 'i am not at all bad', 'i am not at all happy', 'this is not at all sad', 'this is not at all happy', 'i am good right now', 'i am bad right now', 'this is bad right now', 'i am sad right now', 'i was good earlier', 'i was happy earlier', 'i was bad earlier', 'i was sad earlier', 'i am very bad right now', 'this is very good right now', 'this is very sad right now', 'this was bad earlier', 'this was very good earlier', 'this was very bad earlier', 'this was very happy earlier', 'this was very sad earlier', 'i was good and not bad earlier', 'i was not good and not happy earlier', 'i am not at all bad or sad right now', 'i am not at all good or happy right now', 'this was not happy and not good earlier', 'i am joyful', 'this is excellent', 'i am miserable', 'this is awful', 'i am ecstatic', 'this is terrible', 'i am delighted', 'this is dreadful', 'i am elated', 'this is horrendous', 'i am not joyful', 'this is not excellent', 'i am not miserable', 'this is not awful', 'i am not ecstatic', 'this is not terrible', 'i am not delighted', 'this is not dreadful', 'i am not elated', 'this is not horrendous', 'i am very joyful', 'this is very excellent', 'i am very miserable', 'this is very awful', 'i am very ecstatic', 'this is very terrible', 'i am very delighted', 'this is very dreadful', 'i am very elated', 'this is very horrendous', 'i am joyful right now', 'this is excellent right now', 'i am miserable right now', 'this is awful right now', 'i am ecstatic right now', 'this is terrible right now', 'i am delighted right now', 'this is dreadful right now', 'i am elated right now', 'this is horrendous right now', 'i was joyful earlier', 'this was excellent earlier', 'i was miserable earlier', 'this was awful earlier', 'i was ecstatic earlier', 'this was terrible earlier', 'i was delighted earlier', 'this was dreadful earlier', 'i was elated earlier', 'this was horrendous earlier', 'i am joyful and ecstatic', 'this is excellent and delightful', 'i am miserable and dreadful', 'this is awful and horrendous', 'i am joyful and not miserable', 'this is excellent and not awful', 'i am ecstatic and not dreadful', 'this is delightful and not terrible', 'i am elated and not horrendous', 'this is very excellent and not awful', 'i am very joyful and not miserable', 'this is very dreadful and not delightful', 'i am very ecstatic and not horrendous', 'this is very terrible and not excellent', 'i am joyful and ecstatic right now', 'this is excellent and delightful right now', 'i am miserable and dreadful right now', 'this is awful and horrendous right now', 'i was joyful and not miserable earlier', 'this was excellent and not awful earlier', 'i was ecstatic and not dreadful earlier', 'this was delightful and not terrible earlier', 'i was elated and not horrendous earlier', 'i am not at all miserable', 'this is not at all awful', 'i am not at all ecstatic', 'this is not at all terrible', 'i am not at all delighted', 'this is not at all dreadful', 'i am not at all elated', 'this is not at all horrendous', 'i was joyful and delighted earlier', 'this was excellent and ecstatic earlier', 'i was miserable and dreadful earlier', 'this was awful and horrendous earlier', 'i am joyful but not ecstatic', 'this is excellent but not delightful', 'i am miserable but not dreadful', 'this is awful but not terrible', 'i am delighted but not joyful', 'this is dreadful but not horrendous', 'i am ecstatic but not elated', 'this is horrendous but not terrible'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30unique words found\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set([w for text in train_data.keys() for w in text.split(\" \") ]))\n",
    "vocab_size = len(vocab)\n",
    "print(f'{vocab_size}unique words found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 'not')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w2idx = {w:i for i,w in enumerate(vocab)} # lookup table\n",
    "idx2w = {i:w for i,w in enumerate(vocab)} # loopup table\n",
    "w2idx[\"earlier\"], idx2w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 30, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def word2vec(input_text:str):\n",
    "    \"\"\"\n",
    "    - simply one-hot-encoding\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for word in input_text.split(\" \"):\n",
    "        vector = np.zeros((vocab_size,1))\n",
    "        vector[w2idx[word]] = 1\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "vec = word2vec(\"very good very delighted\")\n",
    "np.array(vec).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in vanilla RNN , we have 3 weights(xh,hh,hy) and 2 biases (bh,by) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets build the forward phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999996]\n",
      " [0.50000004]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "class VanillaRNN:\n",
    "    def __init__(self,input_size,output_size,hidden_size=64):\n",
    "        # weights \n",
    "        self.Whh = np.random.randn(hidden_size,hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size,input_size)/1000\n",
    "        self.Why = np.random.randn(output_size,hidden_size)/1000\n",
    "\n",
    "        # biases\n",
    "        self.bh = np.zeros((hidden_size,1))\n",
    "        self.by = np.zeros((output_size,1))\n",
    "\n",
    "    def forward(self,inputs:List[List]):\n",
    "        \"\"\"\n",
    "        performs forward pass for the given list of input vectors\n",
    "        returns the final output and the hidden state\n",
    "        designed for many to one problem task, so produced single, final output.\n",
    "        \"\"\"\n",
    "        # initialize the hidden state chain : first hidden state\n",
    "        # since there is no previous h\n",
    "        h = np.zeros((self.Whh.shape[0],1))\n",
    "\n",
    "        for i,x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh@x + self.Whh@h + self.bh)\n",
    "        \n",
    "        y = self.Why@h + self.by\n",
    "\n",
    "        return y,h\n",
    "    \n",
    "\n",
    "\n",
    "def softmax(xs):\n",
    "  \"\"\"to convert the output logits to prob.\"\"\"\n",
    "  return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "rnn = VanillaRNN(vocab_size, 2) # we want to generate 1 and -1 logits.\n",
    "\n",
    "inputs = word2vec('i am very good and delighted')\n",
    "out, h = rnn.forward(inputs)\n",
    "probs = softmax(out)\n",
    "print(probs) # [[0.50000095], [0.49999905]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass for training\n",
    "- since our task is classification, lets use cross-entropy loss\n",
    "    which is -ln(p)\n",
    "\n",
    "- y : raw output from VanillaRNN\n",
    "- p : final probs = softmax(y)\n",
    "- c : correct class\n",
    "- L : cross-entropy loss\n",
    "- Wxh,Whh,Why : associated weights\n",
    "- bh,by : associated biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to proceed, we need to have access to last_inputs and last hidden states\n",
    "\n",
    "# GRADIENT of the Loss: dL/dy\n",
    "\"\"\"\n",
    "L = -ln(p_c) = -ln(softmax(y_c))\n",
    "dL/dy : \n",
    "{\n",
    "    pi   if  i!=c\n",
    "    pi-1 if  i==c\n",
    "}\n",
    "For example, if we have p=[0.2,0.2,0.6] and the correct class is c=0, \n",
    "then we'd get ∂L∂y = [ -0.8 , -0.8 , 0.6 ]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self,input_size,output_size,hidden_size=64):\n",
    "        # weights \n",
    "        self.Whh = np.random.randn(hidden_size,hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size,input_size)/1000\n",
    "        self.Why = np.random.randn(output_size,hidden_size)/1000\n",
    "\n",
    "        # biases\n",
    "        self.bh = np.zeros((hidden_size,1))\n",
    "        self.by = np.zeros((output_size,1))\n",
    "\n",
    "    def forward(self,inputs:List[List]):\n",
    "        \"\"\"\n",
    "        performs forward pass for the given list of input vectors\n",
    "        returns the final output and the hidden state\n",
    "        \"\"\"\n",
    "        # initialize the hidden state chain : first hidden state\n",
    "        # since there is no previous h\n",
    "        h = np.zeros((self.Whh.shape[0],1))\n",
    "\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0:h}\n",
    "\n",
    "        # hidden state chain\n",
    "        for i,x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh@x + self.Whh@h + self.bh)\n",
    "            self.last_hs.update({i+1:h})\n",
    "\n",
    "        y = self.Why@h + self.by\n",
    "        #print(self.last_hs)\n",
    "        return y,h\n",
    "    \n",
    "\n",
    "    def backpropagate(self,d_y, learning_rate = 2e-2):\n",
    "        \"\"\"\n",
    "        performs backward pass\n",
    "        d_y : calculated loss\n",
    "        \"\"\"\n",
    "        \n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # Calculate dL/dWhy and dL/dby.\n",
    "        d_Why = d_y @ self.last_hs[n].T\n",
    "        d_by = d_y\n",
    "\n",
    "        # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape) # or use zeros_like\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        d_h = self.Why.T @ d_y\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h) # related to tanh\n",
    "\n",
    "            #---GRADIENT ACCUMULATION---#\n",
    "            \n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += temp\n",
    "\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += temp @ self.last_hs[t].T\n",
    "\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += temp @ self.last_inputs[t].T\n",
    "\n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ temp\n",
    "\n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using gradient descent.\n",
    "        self.Whh -= learning_rate * d_Whh\n",
    "        self.Wxh -= learning_rate * d_Wxh\n",
    "        self.Why -= learning_rate * d_Why\n",
    "        self.bh -= learning_rate * d_bh\n",
    "        self.by -= learning_rate * d_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaRNN(vocab_size, 2,hidden_size=64)\n",
    "\n",
    "for x,y in train_data.items():\n",
    "    inputs = word2vec(x)\n",
    "    target = int(y)\n",
    "\n",
    "    # forward pass \n",
    "    out, _ = model.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "\n",
    "    # calculate dL/dy\n",
    "    probs[target] -=1 \n",
    "    dL_dy = probs \n",
    "    # backward pass \n",
    "    model.backpropagate(dL_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resource: https://victorzhou.com/blog/intro-to-rnns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaRNN(vocab_size, 2,hidden_size=64)\n",
    "import random\n",
    "def processData(data, backprop=True):\n",
    "\n",
    "    '''\n",
    "    Returns the RNN's loss and accuracy for the given data.\n",
    "    - data is a dictionary mapping text to True or False.\n",
    "    - backprop determines if the backward phase should be run.\n",
    "    '''\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = word2vec(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # Forward\n",
    "        out, _ = model.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "        \n",
    "        # Calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "\n",
    "        if backprop:\n",
    "            # Build dL/dy\n",
    "            d_L_d_y = probs\n",
    "            d_L_d_y[target] -= 1\n",
    "\n",
    "            # Backward\n",
    "            model.backpropagate(d_L_d_y)\n",
    "\n",
    "    return loss / len(data), num_correct / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Whh[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1\n",
      "Train:\tLoss 0.695 | Accuracy: 0.510\n",
      "Test:\tLoss 0.697 | Accuracy: 0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayhan\\AppData\\Local\\Temp\\ipykernel_19784\\2657295315.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
      "C:\\Users\\ayhan\\AppData\\Local\\Temp\\ipykernel_19784\\2657295315.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 101\n",
      "Train:\tLoss 0.660 | Accuracy: 0.603\n",
      "Test:\tLoss 0.701 | Accuracy: 0.471\n",
      "--- Epoch 201\n",
      "Train:\tLoss 0.090 | Accuracy: 0.974\n",
      "Test:\tLoss 0.336 | Accuracy: 0.943\n",
      "--- Epoch 301\n",
      "Train:\tLoss 0.004 | Accuracy: 1.000\n",
      "Test:\tLoss 0.303 | Accuracy: 0.929\n",
      "--- Epoch 401\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.310 | Accuracy: 0.943\n",
      "--- Epoch 501\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.325 | Accuracy: 0.957\n",
      "--- Epoch 601\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.320 | Accuracy: 0.971\n",
      "--- Epoch 701\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.349 | Accuracy: 0.971\n",
      "--- Epoch 801\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.346 | Accuracy: 0.957\n",
      "--- Epoch 901\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.403 | Accuracy: 0.943\n",
      "--- Epoch 1001\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.362 | Accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1001):\n",
    "  train_loss, train_acc = processData(train_data)\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    print('--- Epoch %d' % (epoch + 1))\n",
    "    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "    test_loss, test_acc = processData(test_data, backprop=False)\n",
    "    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_text):\n",
    "    \n",
    "    inputs = word2vec(input_text)\n",
    "    \n",
    "    # Forward\n",
    "    out, _ = model.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "    print(probs)\n",
    "    return bool(np.argmax(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99476383]\n",
      " [0.00523617]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('this is not at all joyful'), test_data['this is not at all joyful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00856183]\n",
      " [0.99143817]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = predict('this is joyful at all')\n",
    "print(\"NEGATIVE\") if cls==0 else \"POSITIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99999534e-01]\n",
      " [4.65783008e-07]]\n",
      "[[0.01279713]\n",
      " [0.98720287]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('this is not joyful at all'), predict('this is not bad at all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dive_2_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
